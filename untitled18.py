# -*- coding: utf-8 -*-
"""Untitled18.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_uRltrFLCdWrqEnXcSUkdJOd9Cmwx86_
"""

#showing info on the VM used
print('*******CPU Info*******')
get_ipython().system('1scpc')
print('*******RAM Info*******')
get_ipython().system('free -g')

# Installing spark
!sudo apt-get update

#installing Java development kit (JDK)
# Creating Java runtime environment (JRE)
!sudo apt install default-jdk

#Showing Java version 
!java -version

#Downloading Apache spark files
!wget https://archive.apache.org/dist/spark/spark-3.3.2/spark-3.3.2-bin-hadoop2.7.tgz

!tar xvf spark-3.0.3-bin-hadoop2.7.tgz

!1s

#installing findspark and pyspark
#findspark makes pyspark importable as a regular library
!pip install findspark
!pip install pyspark

#setting environment path
import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-11-openjdk-amd64"
os.environ["SPARK_HOME"] = "/content/spark-3.3.2-bin-hadoop2.7"



#
cwd = os.getcwd()
print("current working directory:", cwd)

#running a local sparksession
import findspark
findspark.init()
from pyspark.sql import SparkSession


#creating a sparksession
spark = SparkSession.builder.master("local[*]").getOrCreate()

!pyspark

df = spark.read.csv('/content/SuperStoreOrdersdatanew.csv', inferSchema=True, header=True)
print("DataFrame Shape: ", (df.count(), len(df.columns)))
df.dtypes

df.show(5)

df.columns

df.printSchema()

#finding the count of missing data
from pyspark.sql.functions import when, count, col, isnan
df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df.columns]).show()

df.describe('profit').show()

df.describe().show()

from pyspark.sql.functions import sum,avg,max,count

#PySpark groupby agg
df.groupBy("year") \
    .agg(count("*").alias("count")) \
    .show(truncate=False)

#PySpark groupby agg
df.groupBy("sales") \
    .agg(count("*").alias("count")) \
    .show(truncate=False)

#showing avaerage sales per year
df.groupBy("year").agg({"sales": "mean"}).show(5)



#Selection and summary of all numeric variables

num_cols = [col[0] for col in df.dtypes if col[1] != 'string']
num_cols
# df_road.select(num_cols).describe().show()

#We are using two very popular viz libraries matplotlib and seaborn
import matplotlib.pyplot as plt
import seaborn as sns

#Plot 1: Correlation Heatmap
plt.figure(figsize=(30,15))
corrMatrix= df.toPandas().corr(method='pearson') 
#Please note above how the Spark df is easily transformed into a Pandas dataframe
heatmap = sns.heatmap(corrMatrix,  annot=True, cmap='inferno')
heatmap.set_title('Correlation Heatmap')
plt.show()

#Plot2 again, using seaborn to produce a more appealing visualisation 
df_pandas=df.toPandas()
fig, ax = plt.subplots(figsize=(12,8))
sns.countplot(x="profit", hue="year", data=df_pandas, ax=ax).set(title='profits per year')
plt.xticks(rotation=45)

#Plot2 with a pie chart
df_pandas=df.toPandas()
df_pandas.groupby(['sales']).sum().plot(kind='pie', y='year', autopct='%.2f', explode=(0.05, 0.05, 0.05,0.05, 0.05, 0.05,0.05))